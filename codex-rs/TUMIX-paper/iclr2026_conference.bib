@inproceedings{pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@article{toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@article{codeplan-code-use-llm,
  title={Codeplan: Repository-level coding using llms and planning},
  author={Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B and Shet, Shashank},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={675--698},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{chen2025sets,
  title={Sets: Leveraging self-verification and self-correction for improved test-time scaling},
  author={Chen, Jiefeng and Ren, Jie and Chen, Xinyun and Yang, Chengrun and Sun, Ruoxi and Yoon, Jinsung and Ar{\i}k, Sercan {\"O}},
  journal={arXiv preprint arXiv:2501.19306},
  year={2025}
}

@inproceedings{self-reflection,
  title={Towards mitigating LLM hallucination via self reflection},
  author={Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1827--1843},
  year={2023}
}

@article{LLM-self-correct,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}

@article{gou2023tora,
  title={Tora: A tool-integrated reasoning agent for mathematical problem solving},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2309.17452},
  year={2023}
}

@article{weir2024learning,
  title={Learning to Reason via Program Generation, Emulation, and Search},
  author={Weir, Nathaniel and Khalifa, Muhammad and Qiu, Linlu and Weller, Orion and Clark, Peter},
  journal={arXiv preprint arXiv:2405.16337},
  year={2024}
}

@article{opendevin,
  title={Opendevin: An open platform for ai software developers as generalist agents},
  author={Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},
  journal={arXiv preprint arXiv:2407.16741},
  year={2024}
}

@article{code-based-self-verify,
  title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
  author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
  journal={arXiv preprint arXiv:2308.07921},
  year={2023}
}

@article{chain-of-code,
  title={Chain of code: Reasoning with a language model-augmented code emulator},
  author={Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian},
  journal={arXiv preprint arXiv:2312.04474},
  year={2023}
}

@article{autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

@article{CoT,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{codesteering,
  title={Steering Large Language Models between Code Execution and Textual Reasoning},
  author={Chen, Yongchao and Jhamtani, Harsh and Sharma, Srinagesh and Fan, Chuchu and Wang, Chi},
  journal={arXiv preprint arXiv:2410.03524},
  year={2024}
}

@inproceedings{codesteer,
  title={CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance},
  author={Chen, Yongchao and Hao, Yilun and Liu, Yueying and Zhang, Yang and Fan, Chuchu},
  booktitle={Forty-second International Conference on Machine Learning}
}

@article{R1-Code-Interpreter,
  title={R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning},
  author={Chen, Yongchao and Liu, Yueying and Zhou, Junwei and Hao, Yilun and Wang, Jingquan and Zhang, Yang and Fan, Chuchu},
  journal={arXiv preprint arXiv:2505.21668},
  year={2025}
}

@article{deepseek-r1,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@inproceedings{yang2022re3,
  title={Re3: Generating Longer Stories With Recursive Reprompting and Revision},
  author={Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4393--4479},
  year={2022}
}

@inproceedings{welleck2022generating,
  title={Generating Sequences by Learning to Self-Correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

% Inner Monologue
@article{inner-monologue,
  title={Inner monologue: Embodied reasoning through planning with language models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  journal={arXiv preprint arXiv:2207.05608},
  year={2022}
}

@article{codeio,
  title={CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction},
  author={Li, Junlong and Guo, Daya and Yang, Dejian and Xu, Runxin and Wu, Yu and He, Junxian},
  journal={arXiv preprint arXiv:2502.07316},
  year={2025}
}

@article{search-r1,
  title={Search-r1: Training llms to reason and leverage search engines with reinforcement learning},
  author={Jin, Bowen and Zeng, Hansi and Yue, Zhenrui and Yoon, Jinsung and Arik, Sercan and Wang, Dong and Zamani, Hamed and Han, Jiawei},
  journal={arXiv preprint arXiv:2503.09516},
  year={2025}
}

@article{overthink-o1,
  title={Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{O1-model,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{Deep-Researcher-TTD,
  title={Deep Researcher with Test-Time Diffusion},
  author={Han, Rujun and Chen, Yanfei and CuiZhu, Zoey and Miculicich, Lesly and Sun, Guan and Bi, Yuanjun and Wen, Weiming and Wan, Hui and Wen, Chunfeng and Ma{\^\i}tre, Sol{\`e}ne and others},
  journal={arXiv preprint arXiv:2507.16075},
  year={2025}
}

@article{LLM-monkey,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@inproceedings{scalable-multi-robot,
  title={Scalable multi-robot collaboration with large language models: Centralized or decentralized systems?},
  author={Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={4311--4317},
  year={2024},
  organization={IEEE}
}

@article{camel,
  title={Camel: Communicative agents for" mind" exploration of large language model society},
  author={Li, Guohao and Hammoud, Hasan and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={51991--52008},
  year={2023}
}

@inproceedings{langchain,
  title={Creating large language model applications utilizing langchain: A primer on developing llm apps fast},
  author={Topsakal, Oguzhan and Akinci, Tahir Cetin},
  booktitle={International Conference on Applied Engineering and Natural Sciences},
  volume={1},
  number={1},
  pages={1050--1056},
  year={2023}
}

@article{MOA,
  title={Mixture-of-Agents Enhances Large Language Model Capabilities},
  author={Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  journal={arXiv preprint arXiv:2406.04692},
  year={2024}
}

@article{rethink-MOA,
  title={Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?},
  author={Li, Wenzhe and Lin, Yong and Xia, Mengzhou and Jin, Chi},
  journal={arXiv preprint arXiv:2502.00674},
  year={2025}
}

@article{gemini-2.5-pro,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@article{DEI,
  title={Diversity empowers intelligence: Integrating expertise of software engineering agents},
  author={Zhang, Kexun and Yao, Weiran and Liu, Zuxin and Feng, Yihao and Liu, Zhiwei and Murthy, Rithesh and Lan, Tian and Li, Lei and Lou, Renze and Xu, Jiacheng and others},
  journal={arXiv preprint arXiv:2408.07060},
  year={2024}
}

@article{symbolic-MOE,
  title={Symbolic mixture-of-experts: Adaptive skill-based routing for heterogeneous reasoning},
  author={Chen, Justin Chih-Yao and Yun, Sukwon and Stengel-Eskin, Elias and Chen, Tianlong and Bansal, Mohit},
  journal={arXiv preprint arXiv:2503.05641},
  year={2025}
}

@article{lessons-learned,
  title={Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve},
  author={Liu, Yuanzhe and Deng, Ryan and Kaler, Tim and Chen, Xuhao and Leiserson, Charles E and Ma, Yao and Chen, Jie},
  journal={arXiv preprint arXiv:2505.23946},
  year={2025}
}

@article{GSA:Llms-aggregating-own-responses,
  title={Llms can generate a better answer by aggregating their own responses},
  author={Li, Zichong and Feng, Xinyu and Cai, Yuheng and Zhang, Zixuan and Liu, Tianyi and Liang, Chen and Chen, Weizhu and Wang, Haoyu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2503.04104},
  year={2025}
}

@article{HLE,
  title={Humanity's last exam},
  author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and others},
  journal={arXiv preprint arXiv:2501.14249},
  year={2025}
}

@inproceedings{gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{LLM-mixture-of-thoughts-cost-efficient-reasoning,
  title={Large language model cascades with mixture of thoughts representations for cost-efficient reasoning},
  author={Yue, Murong and Zhao, Jie and Zhang, Min and Du, Liang and Yao, Ziyu},
  journal={arXiv preprint arXiv:2310.03094},
  year={2023}
}

@inproceedings{GAIA,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{Search-O1,
  title={Search-o1: Agentic search-enhanced large reasoning models},
  author={Li, Xiaoxi and Dong, Guanting and Jin, Jiajie and Zhang, Yuyao and Zhou, Yujia and Zhu, Yutao and Zhang, Peitian and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2501.05366},
  year={2025}
}

@article{llm+code=commense-learner,
  title={Language models of code are few-shot commonsense learners},
  author={Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
  journal={arXiv preprint arXiv:2210.07128},
  year={2022}
}

@article{Program-of-thoughts-prompting,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{fu2025deep,
  title={Deep Think with Confidence},
  author={Fu, Yichao and Wang, Xuewei and Tian, Yuandong and Zhao, Jiawei},
  journal={arXiv preprint arXiv:2508.15260},
  year={2025}
}

@article{LATS,
  title={Language agent tree search unifies reasoning acting and planning in language models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  journal={arXiv preprint arXiv:2310.04406},
  year={2023}
}

@article{Tree-of-thought,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{retool,
  title={Retool: Reinforcement learning for strategic tool use in llms},
  author={Feng, Jiazhan and Huang, Shijue and Qu, Xingwei and Zhang, Ge and Qin, Yujia and Zhong, Baoquan and Jiang, Chengquan and Chi, Jinxin and Zhong, Wanjun},
  journal={arXiv preprint arXiv:2504.11536},
  year={2025}
}

@article{deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{SWE-RL,
  title={Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution},
  author={Wei, Yuxiang and Duchenne, Olivier and Copet, Jade and Carbonneaux, Quentin and Zhang, Lingming and Fried, Daniel and Synnaeve, Gabriel and Singh, Rishabh and Wang, Sida I},
  journal={arXiv preprint arXiv:2502.18449},
  year={2025}
}

@inproceedings{yilun-LLM-debate-improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{chai2025scimaster,
  title={SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?},
  author={Chai, Jingyi and Tang, Shuo and Ye, Rui and Du, Yuwen and Zhu, Xinyu and Zhou, Mengcheng and Wang, Yanfeng and Zhang, Yuzhi and Zhang, Linfeng and Chen, Siheng and others},
  journal={arXiv preprint arXiv:2507.05241},
  year={2025}
}

@article{toolrl,
  title={ToolRL: Reward is All Tool Learning Needs},
  author={Qian, Cheng and Acikgoz, Emre Can and He, Qi and Wang, Hongru and Chen, Xiusi and Hakkani-T{\"u}r, Dilek and Tur, Gokhan and Ji, Heng},
  journal={arXiv preprint arXiv:2504.13958},
  year={2025}
}

@article{torl,
  title={Torl: Scaling tool-integrated rl},
  author={Li, Xuefeng and Zou, Haoyang and Liu, Pengfei},
  journal={arXiv preprint arXiv:2503.23383},
  year={2025}
}