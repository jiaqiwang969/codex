\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bairi et~al.(2024)Bairi, Sonwane, Kanade, Iyer, Parthasarathy, Rajamani, Ashok, and Shet]{codeplan-code-use-llm}
Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B~Ashok, and Shashank Shet.
\newblock Codeplan: Repository-level coding using llms and planning.
\newblock \emph{Proceedings of the ACM on Software Engineering}, 1\penalty0 (FSE):\penalty0 675--698, 2024.

\bibitem[Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, R{\'e}, and Mirhoseini]{LLM-monkey}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V Le, Christopher R{\'e}, and Azalia Mirhoseini.
\newblock Large language monkeys: Scaling inference compute with repeated sampling.
\newblock \emph{arXiv preprint arXiv:2407.21787}, 2024.

\bibitem[Chai et~al.(2025)Chai, Tang, Ye, Du, Zhu, Zhou, Wang, Zhang, Zhang, Chen, et~al.]{chai2025scimaster}
Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Yuzhi Zhang, Linfeng Zhang, Siheng Chen, et~al.
\newblock Scimaster: Towards general-purpose scientific ai agents, part i. x-master as foundation: Can we lead on humanity's last exam?
\newblock \emph{arXiv preprint arXiv:2507.05241}, 2025.

\bibitem[Chen et~al.(2025{\natexlab{a}})Chen, Ren, Chen, Yang, Sun, Yoon, and Ar{\i}k]{chen2025sets}
Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Jinsung Yoon, and Sercan~{\"O} Ar{\i}k.
\newblock Sets: Leveraging self-verification and self-correction for improved test-time scaling.
\newblock \emph{arXiv preprint arXiv:2501.19306}, 2025{\natexlab{a}}.

\bibitem[Chen et~al.(2025{\natexlab{b}})Chen, Yun, Stengel-Eskin, Chen, and Bansal]{symbolic-MOE}
Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, and Mohit Bansal.
\newblock Symbolic mixture-of-experts: Adaptive skill-based routing for heterogeneous reasoning.
\newblock \emph{arXiv preprint arXiv:2503.05641}, 2025{\natexlab{b}}.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{Program-of-thoughts-prompting}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}, 2022.

\bibitem[Chen et~al.()Chen, Hao, Liu, Zhang, and Fan]{codesteer}
Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, and Chuchu Fan.
\newblock Codesteer: Symbolic-augmented language models via code/text guidance.
\newblock In \emph{Forty-second International Conference on Machine Learning}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Arkin, Zhang, Roy, and Fan]{scalable-multi-robot}
Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu Fan.
\newblock Scalable multi-robot collaboration with large language models: Centralized or decentralized systems?
\newblock In \emph{2024 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  4311--4317. IEEE, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Jhamtani, Sharma, Fan, and Wang]{codesteering}
Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, and Chi Wang.
\newblock Steering large language models between code execution and textual reasoning.
\newblock \emph{arXiv preprint arXiv:2410.03524}, 2024{\natexlab{b}}.

\bibitem[Comanici et~al.(2025)Comanici, Bieber, Schaekermann, Pasupat, Sachdeva, Dhillon, Blistein, Ram, Zhang, Rosen, et~al.]{Gemini-2.5-Pro}
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et~al.
\newblock Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
\newblock \emph{arXiv preprint arXiv:2507.06261}, 2025.

\bibitem[Du et~al.()Du, Li, Torralba, Tenenbaum, and Mordatch]{yilun-LLM-debate-improving}
Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[Feng et~al.(2025)Feng, Huang, Qu, Zhang, Qin, Zhong, Jiang, Chi, and Zhong]{retool}
Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge~Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong.
\newblock Retool: Reinforcement learning for strategic tool use in llms.
\newblock \emph{arXiv preprint arXiv:2504.11536}, 2025.

\bibitem[Fu et~al.(2025)Fu, Wang, Tian, and Zhao]{fu2025deep}
Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao.
\newblock Deep think with confidence.
\newblock \emph{arXiv preprint arXiv:2508.15260}, 2025.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10764--10799. PMLR, 2023.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Zhang, Xu, Zhu, Ma, Wang, Bi, et~al.]{deepseek-r1}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}, 2025.

\bibitem[Jaech et~al.(2024)Jaech, Kalai, Lerer, Richardson, El-Kishky, Low, Helyar, Madry, Beutel, Carney, et~al.]{O1-model}
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et~al.
\newblock Openai o1 system card.
\newblock \emph{arXiv preprint arXiv:2412.16720}, 2024.

\bibitem[Ji et~al.(2023)Ji, Yu, Xu, Lee, Ishii, and Fung]{self-reflection}
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.
\newblock Towards mitigating llm hallucination via self reflection.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  1827--1843, 2023.

\bibitem[Jin et~al.(2025)Jin, Zeng, Yue, Yoon, Arik, Wang, Zamani, and Han]{search-r1}
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han.
\newblock Search-r1: Training llms to reason and leverage search engines with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2503.09516}, 2025.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Liang, Zeng, Chen, Hausman, Sadigh, Levine, Fei-Fei, Xia, and Ichter]{chain-of-code}
Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li~Fei-Fei, Fei Xia, and Brian Ichter.
\newblock Chain of code: Reasoning with a language model-augmented code emulator.
\newblock \emph{arXiv preprint arXiv:2312.04474}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Hammoud, Itani, Khizbullin, and Ghanem]{camel}
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
\newblock Camel: Communicative agents for" mind" exploration of large language model society.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 51991--52008, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2025{\natexlab{a}})Li, Lin, Xia, and Jin]{rethink-MOA}
Wenzhe Li, Yong Lin, Mengzhou Xia, and Chi Jin.
\newblock Rethinking mixture-of-agents: Is mixing different large language models beneficial?
\newblock \emph{arXiv preprint arXiv:2502.00674}, 2025{\natexlab{a}}.

\bibitem[Li et~al.(2025{\natexlab{b}})Li, Dong, Jin, Zhang, Zhou, Zhu, Zhang, and Dou]{Search-O1}
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.
\newblock Search-o1: Agentic search-enhanced large reasoning models.
\newblock \emph{arXiv preprint arXiv:2501.05366}, 2025{\natexlab{b}}.

\bibitem[Li et~al.(2025{\natexlab{c}})Li, Zou, and Liu]{torl}
Xuefeng Li, Haoyang Zou, and Pengfei Liu.
\newblock Torl: Scaling tool-integrated rl.
\newblock \emph{arXiv preprint arXiv:2503.23383}, 2025{\natexlab{c}}.

\bibitem[Li et~al.(2025{\natexlab{d}})Li, Feng, Cai, Zhang, Liu, Liang, Chen, Wang, and Zhao]{GSA:Llms-aggregating-own-responses}
Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, and Tuo Zhao.
\newblock Llms can generate a better answer by aggregating their own responses.
\newblock \emph{arXiv preprint arXiv:2503.04104}, 2025{\natexlab{d}}.

\bibitem[Madaan et~al.(2022)Madaan, Zhou, Alon, Yang, and Neubig]{llm+code=commense-learner}
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.
\newblock Language models of code are few-shot commonsense learners.
\newblock \emph{arXiv preprint arXiv:2210.07128}, 2022.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Phan et~al.(2025)Phan, Gatti, Han, Li, Hu, Zhang, Zhang, Shaaban, Ling, Shi, et~al.]{HLE}
Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo~Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et~al.
\newblock Humanity's last exam.
\newblock \emph{arXiv preprint arXiv:2501.14249}, 2025.

\bibitem[Qian et~al.(2025)Qian, Acikgoz, He, Wang, Chen, Hakkani-T{\"u}r, Tur, and Ji]{toolrl}
Cheng Qian, Emre~Can Acikgoz, Qi~He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T{\"u}r, Gokhan Tur, and Heng Ji.
\newblock Toolrl: Reward is all tool learning needs.
\newblock \emph{arXiv preprint arXiv:2504.13958}, 2025.

\bibitem[Rein et~al.(2024)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark.
\newblock In \emph{First Conference on Language Modeling}, 2024.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu, et~al.]{deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK~Li, Y~Wu, et~al.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Topsakal \& Akinci(2023)Topsakal and Akinci]{langchain}
Oguzhan Topsakal and Tahir~Cetin Akinci.
\newblock Creating large language model applications utilizing langchain: A primer on developing llm apps fast.
\newblock In \emph{International Conference on Applied Engineering and Natural Sciences}, volume~1, pp.\  1050--1056, 2023.

\bibitem[Wang et~al.(2024)Wang, Wang, Athiwaratkun, Zhang, and Zou]{MOA}
Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce~Zhang, and James Zou.
\newblock Mixture-of-agents enhances large language model capabilities.
\newblock \emph{arXiv preprint arXiv:2406.04692}, 2024.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{CoT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wei et~al.(2025)Wei, Duchenne, Copet, Carbonneaux, Zhang, Fried, Synnaeve, Singh, and Wang]{SWE-RL}
Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida~I Wang.
\newblock Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution.
\newblock \emph{arXiv preprint arXiv:2502.18449}, 2025.

\bibitem[Weir et~al.(2024)Weir, Khalifa, Qiu, Weller, and Clark]{weir2024learning}
Nathaniel Weir, Muhammad Khalifa, Linlu Qiu, Orion Weller, and Peter Clark.
\newblock Learning to reason via program generation, emulation, and search.
\newblock \emph{arXiv preprint arXiv:2405.16337}, 2024.

\bibitem[Welleck et~al.(2022)Welleck, Lu, West, Brahman, Shen, Khashabi, and Choi]{welleck2022generating}
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi.
\newblock Generating sequences by learning to self-correct.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Zhang, Zhu, Li, Jiang, Zhang, and Wang]{autogen}
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li~Jiang, Xiaoyun Zhang, and Chi Wang.
\newblock Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
\newblock \emph{arXiv preprint arXiv:2308.08155}, 2023.

\bibitem[Yang et~al.(2022)Yang, Tian, Peng, and Klein]{yang2022re3}
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein.
\newblock Re3: Generating longer stories with recursive reprompting and revision.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  4393--4479, 2022.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{Tree-of-thought}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Yao, Liu, Feng, Liu, Murthy, Lan, Li, Lou, Xu, et~al.]{DEI}
Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, et~al.
\newblock Diversity empowers intelligence: Integrating expertise of software engineering agents.
\newblock \emph{arXiv preprint arXiv:2408.07060}, 2024.

\bibitem[Zhou et~al.(2023)Zhou, Wang, Lu, Shi, Luo, Qin, Lu, Jia, Song, Zhan, et~al.]{code-based-self-verify}
Aojun Zhou, Ke~Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et~al.
\newblock Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification.
\newblock \emph{arXiv preprint arXiv:2308.07921}, 2023.

\end{thebibliography}
