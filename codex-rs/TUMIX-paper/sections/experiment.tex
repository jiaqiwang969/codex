\section{Experiments}
\subsection{Experimental settings}
\textbf{Benchmarks}\quad For a comprehensive evaluation and comparison across methods, we conduct experiments on three representative benchmarks that demand extensive reasoning and planning, particularly the ability to effectively leverage Code Interpreter and Search. \textbf{HLE}~\citep{HLE} consists of 2,500 highly challenging questions spanning diverse subject areas, including mathematics, biology, engineering, computer science, and the social sciences. It is designed as a final, closed-ended benchmark of broad academic capability. We evaluate both its text-only and multimodal subsets. In the following sections, we primarily use HLE to study different mechanisms, as it contains a large number of questions spanning diverse domains and is the most challenging benchmark. \textbf{GPQA}~\citep{gpqa} is a multiple-choice dataset authored by domain experts in biology, physics, and chemistry. We focus on its most widely used subset, \textbf{GPQA Diamond}, which contains 198 of the most challenging and carefully curated questions. Finally, \textbf{AIME 2024\&2025} comprises 60 problems from the 2024 and 2025 AIME exams, a notoriously difficult high school mathematics competition. All reported results are averaged over three independent runs.

\textbf{Baselines/\texttt{TUMIX} ablations and extensions/Test models}\quad As shown in Appendix~Table~\ref{tab:baselines}, we compare against the following methods:  
(1) \texttt{Majority-Vote}~\citep{LLM-monkey};(2) \texttt{GSA}~\citep{GSA:Llms-aggregating-own-responses}; (3) \texttt{Self-Reflection}~\citep{self-reflection}; (4) \texttt{SETS}~\citep{chen2025sets}; (5) \texttt{Self-MoA}~\citep{rethink-MOA}; (6) \texttt{Symbolic-MoE}~\citep{symbolic-MOE}; (7) \texttt{DEI}~\citep{DEI}; (8) \texttt{SciMaster} \citep{chai2025scimaster}. For baselines (1)–(4), we use the \texttt{CS} agent, which has full access to both Code Interpreter and Search and achieves relatively high first-round accuracy (Appendix~Table~\ref{table: acc round evolution}). For baselines (5)–(7), we select agents following their original methods. For \texttt{SciMaster}, we retain the original prompts and agents to ensure consistency with published results. We match the total inference counts of all baselines to \texttt{TUMIX} by adjusting agent numbers and sampling repetitions for fair comparison. All the baselines have full access to Code Interpreter and Search. We evaluate \texttt{TUMIX} variants with different design choices, either to ablate framework components or to introduce improvements over existing \texttt{TUMIX}, as shown in Appendix~Table~\ref{tab:tumix_variants}. \texttt{TUMIX+} uses higher inference costs to test scaling effects, while other variants consume nearly the same inference and token counts. We evaluate our methods on the reasoning LLM Gemini-2.5-Pro and Gemini-2.5-Flash.

\textbf{Evaluation protocol}\quad Answers are evaluated against ground-truth solutions, with Gemini-2.5-Pro assisting in normalizing answer formats when necessary or serving directly as the judge for answer comparison. In cases where the model outputs code as the final answer, we extract the code using predefined algorithms and execute it to produce the final result. To avoid infinite loops, all code execution whether during intermediate or final rounds is limited to 60 seconds. If execution exceeds this limit, a ``code runtime error'' is returned to the model for regeneration in intermediate rounds; in the final round, the task is marked as a failure. We report success rate as the primary evaluation metric. In addition to task performance, we also analyze token usage and inference time for each method in later sections.

\subsection{Overall better performance}

\begin{comment}
\begin{table*}[ht]
\vspace{-2mm}
\caption{Experimental results of baseline and proposed methods on HLE, GPQA, and AIME 24\&25. Except for the single-inference \texttt{w/o TTS} and the scaled-up \texttt{TUMIX+}, all methods use comparable inference costs for scaling. For some methods, Gemini-2.5-Pro's HLE results are used to select agents within their agentic framework. In these cases, the method has prior knowledge of HLE and the results cannot be strictly regarded as test performance. Such cases are marked with \textsuperscript{*} in the HLE results. All the values are the average of three repetitive runs.}
\label{table: overall results}
\vskip 0.15in
\begin{center}
\begin{small}
%\begin{scriptsize} % more compact than small
\begin{sc}
\begin{tabular}{lcccccccccyyyz}
\toprule
\multicolumn{1}{c}{Methods} & \multicolumn{9}{c}{\textbf{Baseline Methods}} & \multicolumn{4}{c}{\textbf{Proposed Methods}}\\
\cmidrule(r){2-10} \cmidrule(l){11-14}
\rotatebox{80}{Accuracy \%} & \rotatebox{80}{w/o TTS} &
\rotatebox{80}{Majority Vote} &
\rotatebox{80}{Self-MoA} &
\rotatebox{80}{Symbolic-MoE} & \rotatebox{80}{DEI} & \rotatebox{80}{Self-Reflection} & \rotatebox{80}{SETS} & \rotatebox{80}{SciMaster} & \rotatebox{80}{GSA}  &
\rotatebox{80}{TUMIX} & \rotatebox{80}{TUMIX-FixedR} & \rotatebox{80}{TUMIX-Evolve} & \rotatebox{80}{TUMIX+}\\
\midrule
\multicolumn{1}{c}{} & \multicolumn{13}{c}{\textbf{Gemini-2.5-Pro}}\\
HLE & 21.6 & 28.4 & 29.3\textsuperscript{*} & 29.5\textsuperscript{*} & 29.1\textsuperscript{*} & 23.5 & 27.9 & 26.9 & 28.7 & 32.3 & 32.4 & \textcolor{blue}{32.7\textsuperscript{*}} & \textcolor{magenta}{34.1} \\
GPQA & 84.6 & 84.9 & 85.5 & 86.7 & 86.0 & 84.9 & 85.3 & 86.9 & 85.8 & 87.9 & 86.8 & \textcolor{blue}{88.1} & \textcolor{magenta}{88.3} \\
AIME 24\&25 & 87.3 & 94.3 & 94.7 & 94.7 & 95.0 & 88.3 & 94.7 & 94.1 & 93.7 & \textcolor{blue}{96.7} & 95.6 & \textcolor{blue}{96.7} & \textcolor{magenta}{96.7} \\
\textbf{Ave. Norm.} & \textbf{64.5} & \textbf{69.2} & \textbf{69.8} & \textbf{70.3} & \textbf{70.0} & \textbf{65.6} & \textbf{69.3} & \textbf{69.3} & \textbf{69.4} & \textbf{72.3} & \textbf{71.6} & \textcolor{blue}{\textbf{72.5}} & \textcolor{magenta}{\textbf{73.0}} \\
\midrule
\multicolumn{1}{c}{} & \multicolumn{13}{c}{\textbf{Gemini-2.5-Flash}}\\
HLE & 9.7 & 17.9 & 18.2 & 18.5 & 19.3 & 10.4 & 18.5 & 18.0 & 17.4 & 21.2 & 20.9 & \textcolor{blue}{21.9} & \textcolor{magenta}{23.1}\\
GPQA & 50.0 & 63.1 & 65.4 & 64.1 & 64.9 & 53.2 & 63.2 & 67.9 & 62.6 & 77.3 & 76.8 & \textcolor{blue}{79.8} & \textcolor{magenta}{82.1} \\
AIME 24\&25 & 70.0 & 80.0 & 80.3 & 80.7 & 82.3 & 72.3 & 74.0 & 79.1 & 79.7 & 83.3 & 83.3 & \textcolor{blue}{86.7} & \textcolor{magenta}{86.7} \\
\textbf{Ave. Norm.} & \textbf{43.2} & \textbf{53.7} & \textbf{54.7} & \textbf{54.4} & \textbf{55.5} & \textbf{45.3} & \textbf{51.9} & \textbf{55.0} & \textbf{53.2} & \textbf{60.6} & \textbf{60.3} & \textcolor{blue}{\textbf{62.8}} & \textcolor{magenta}{\textbf{64.0}} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\vspace{-2mm}
\end{table*}
\end{comment}

\begin{table*}[ht]
\caption{Experimental results of baseline and proposed methods on HLE, GPQA, and AIME 24\&25. Except for the single-inference \texttt{w/o TTS} and the scaled-up \texttt{TUMIX+}, all methods use comparable inference costs for scaling. For some methods, Gemini-2.5-Pro's HLE results are used to select agents within their agentic framework. In these cases, the method has prior knowledge of HLE and the results cannot be strictly regarded as test performance. Such cases are marked with \textsuperscript{*} in the HLE results. All the values are the average of three repetitive runs.}
\label{table: overall results}
\vskip 0.15in
\centering
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
\textbf{Methods} & \textbf{HLE} & \textbf{GPQA} & \textbf{AIME 24\&25} & \textbf{Ave. Norm.} \\
\midrule
\multicolumn{5}{c}{\textbf{Gemini-2.5-Pro}}\\
w/o TTS & 21.6 & 84.6 & 87.3 & \textbf{64.5} \\
Majority Vote & 28.4 & 84.9 & 94.3 & \textbf{69.2} \\
Self-MoA & 29.3\textsuperscript{*} & 85.5 & 94.7 & \textbf{69.8} \\
Symbolic-MoE & 29.5\textsuperscript{*} & 86.7 & 94.7 & \textbf{70.3} \\
DEI & 29.1\textsuperscript{*} & 86.0 & 95.0 & \textbf{70.0} \\
Self-Reflection & 23.5 & 84.9 & 88.3 & \textbf{65.6} \\
SETS & 27.9 & 85.3 & 94.7 & \textbf{69.3} \\
SciMaster & 26.9 & 86.9 & 94.1 & \textbf{69.3} \\
GSA & 28.7 & 85.8 & 93.7 & \textbf{69.4} \\
\rowcolor{LightCyan} TUMIX & 32.3 & 87.9 & \textcolor{blue}{96.7} & \textbf{72.3} \\
\rowcolor{LightCyan} TUMIX-FixedR & 32.4 & 86.8 & 95.6 & \textbf{71.6} \\
\rowcolor{LightCyan} TUMIX-Evolve & \textcolor{blue}{32.7\textsuperscript{*}} & \textcolor{blue}{88.1} & \textcolor{blue}{96.7} & \textcolor{blue}{\textbf{72.5}} \\
\rowcolor{softlavender} TUMIX+ & \textcolor{magenta}{34.1} & \textcolor{magenta}{88.3} & \textcolor{magenta}{96.7} & \textcolor{magenta}{\textbf{73.0}} \\
\midrule
\multicolumn{5}{c}{\textbf{Gemini-2.5-Flash}}\\
w/o TTS & 9.7 & 50.0 & 70.0 & \textbf{43.2} \\
Majority Vote & 17.9 & 63.1 & 80.0 & \textbf{53.7} \\
Self-MoA & 18.2 & 65.4 & 80.3 & \textbf{54.7} \\
Symbolic-MoE & 18.5 & 64.1 & 80.7 & \textbf{54.4} \\
DEI & 19.3 & 64.9 & 82.3 & \textbf{55.5} \\
Self-Reflection & 10.4 & 53.2 & 72.3 & \textbf{45.3} \\
SETS & 18.5 & 63.2 & 74.0 & \textbf{51.9} \\
SciMaster & 18.0 & 67.9 & 79.1 & \textbf{55.0} \\
GSA & 17.4 & 62.6 & 79.7 & \textbf{53.2} \\
\rowcolor{LightCyan} TUMIX & 21.2 & 77.3 & 83.3 & \textbf{60.6} \\
\rowcolor{LightCyan} TUMIX-FixedR & 20.9 & 76.8 & 83.3 & \textbf{60.3} \\
\rowcolor{LightCyan} TUMIX-Evolve & \textcolor{blue}{21.9} & \textcolor{blue}{79.8} & \textcolor{blue}{86.7} & \textcolor{blue}{\textbf{62.8}} \\
\rowcolor{softlavender} TUMIX+ & \textcolor{magenta}{23.1} & \textcolor{magenta}{82.1} & \textcolor{magenta}{86.7} & \textcolor{magenta}{\textbf{64.0}} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vskip -0.1in
\vspace{-2mm}
\end{table*}

Table~\ref{table: overall results} shows that \texttt{TUMIX} outperforms all baselines, with average accuracy improvements of 2.0\% and 5.9\% over the best methods using Gemini-2.5-Pro and Gemini-2.5-Flash, respectively. Its superior performance over methods without answer sharing (\texttt{Self-Reflection}, \texttt{SETS}) highlights the importance of answer sharing in multi-round test-time scaling. Comparisons with methods lacking multi-round refinement (\texttt{Majority-Vote}, \texttt{Symbolic-MoE}, \texttt{DEI}, \texttt{GSA}) demonstrate the benefits of refinement, while comparisons with methods lacking agent diversity (\texttt{Self-MoA}, \texttt{SciMaster}) confirm the value of diverse agents. The accuracy improvement of \texttt{SciMaster} on HLE is smaller than reported by the authors. We suspect this discrepancy arises from differences in tools, as their Search and Code Interpreter modules are not open-sourced.