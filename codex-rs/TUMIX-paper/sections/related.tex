\section{Related Work}
\textbf{Code Interpreter and Search}\quad Many benchmark tasks can in fact be better solved through code~\citep{pal} and search~\citep{Search-O1}, and recent work extends coding to reasoning and semantic analysis~\citep{chain-of-code,weir2024learning}. Most prior approaches use either text~\citep{Tree-of-thought} or code~\citep{codeplan-code-use-llm,code-based-self-verify} exclusively as output. Recent work~\citep{codesteering} emphasizes the need to dynamically switch between modalities, proposing CodeSteer~\citep{codesteer} as a guidance model. Extensions with retrieval~\citep{search-r1,Search-O1} and tool use~\citep{toolrl} further improve reasoning, but lack the thorough exploitation of Code Interpreter and Search tools. Leading models such as OpenAI’s ChatGPT Agent, Google’s Gemini-Pro~\citep{Gemini-2.5-Pro}, and XAI’s Grok4 report using code and search at test time to augment reasoning, but without publishing detailed methods. Open work such as ToRL~\citep{torl} and ReTool~\citep{retool} investigates training reasoning models to integrate with Code Interpreters. However, their training and evaluation are limited to math problems, leaving a significant gap from real-world applications that demand effectiveness across broader benchmarks. ToolRL~\citep{toolrl} instead focuses on teaching models to select among multiple tools, where the generated codes and search queries are relative simple and the evaluation tasks require less reasoning capabilities. SciMaster~\citep{chai2025scimaster} samples the same pre-designed tool-use agent five times, then uses other pre-designed agents to critique, refine, and aggregate the answers. This approach shows clear improvement over single-inference text-only baselines, but the extent and manner of tool exploitation remain underexplored. In summary, integrating Code Interpreter and Search into LLM reasoning is essential and challenging. The academic community currently lacks methods and studies that fully exploit the benefits of LLM self-reasoning, code execution, and search, which is the focus of our work.

\textbf{Test-time scaling}\quad LLM self-exploration, reflection, and evaluation can enhance task performance across domains~\citep{yang2022re3, welleck2022generating, madaan2023self}. Models like OpenAI o1~\citep{O1-model} and DeepSeek R1~\citep{deepseek-r1} showcase agentic behavior via Chain-of-Thought (CoT) reasoning and self-reflection, which is learned by RL-based training with rule-based outcome rewards~\citep{deepseekmath,SWE-RL}. Apart from the training-based scaling, many research also explore scaling during LLM inference time by pre-designing prompt and agent frameworks. In these works, multi-agent reasoning has emerged as a promising paradigm for enhancing complex problem-solving and decision-making in AI systems~\citep{autogen,camel,langchain}. Prior work finds gathering the answers from different LLMs improves LLM performance~\citep{yilun-LLM-debate-improving}. Mixture-of-Agents (MoA)~\citep{MOA} further extends this idea by sharing and gathering answer among LLMs. However, Self-MoA~\citep{rethink-MOA} argues that LLM diversity may not be critical since replacing different types of LLMs with the best one achieves better performance. Symbolic-MoE~\citep{symbolic-MOE} further assigns different questions with different specialized LLMs. Instead of using different types of LLMs, many works such as DEI~\citep{DEI}, GSA~\citep{GSA:Llms-aggregating-own-responses}, and SETS~\citep{chen2025sets} employ different agents from the same LLM for extensive test-time scaling, in which the agent types and frameworks are explored~\citep{scalable-multi-robot}. Similar to our work, previous work in test-time scaling also finds the correct answer selection~\citep{LLM-monkey} is the main bottleneck. While previous work in test-time scaling do not incorporate tool-use of Code Interpreter and Search, we study how to utilize test-time scaling methods to better exploit the benefits of each reasoning mode.