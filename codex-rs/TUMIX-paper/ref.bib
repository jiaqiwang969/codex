@inproceedings{huanglarge,
  title={Large Language Models Cannot Self-Correct Reasoning Yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{zheng2024natural,
  title={NATURAL PLAN: Benchmarking LLMs on Natural Language Planning},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Zhang, Hugh and Chen, Xinyun and Chen, Minmin and Nova, Azade and Hou, Le and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and others},
  journal={arXiv preprint arXiv:2406.04520},
  year={2024}
}

@article{livebench,
  author    = {White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and Hegde, Chinmay and LeCun, Yann and Goldstein, Tom and Neiswanger, Willie and Goldblum, Micah},
  title     = {LiveBench: A Challenging, Contamination-Free LLM Benchmark},
  url       = {arXiv preprint arXiv:2406.19314},
  year      = {2024},
}

@inproceedings{wangself,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{weng2023large,
  title={Large Language Models are Better Reasoners with Self-Verification},
  author={Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Shengping and Sun, Bin and Liu, Kang and Zhao, Jun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={2550--2575},
  year={2023}
}

@article{li2024confidence,
  title={Confidence matters: Revisiting intrinsic self-correction capabilities of large language models},
  author={Li, Loka and Chen, Zhenhao and Chen, Guangyi and Zhang, Yixuan and Su, Yusheng and Xing, Eric and Zhang, Kun},
  journal={arXiv preprint arXiv:2402.12563},
  year={2024}
}

@inproceedings{li2024hindsight,
  title={When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models},
  author={Li, Yanhong and Yang, Chenghao and Ettinger, Allyson},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3741--3753},
  year={2024}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@software{penglangfun2023,
author = {Peng, Daiyi},
license = {Apache-2.0},
month = sep,
title = {{Langfun}},
url = {https://github.com/google/langfun},
version = {0.0.1},
year = {2023}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{welleck2024decoding,
  title={From decoding to meta-generation: Inference-time algorithms for large language models},
  author={Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:2406.16838},
  year={2024}
}

@inproceedings{chen2024more,
  title={Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{liang2024improving,
  title={Improving llm reasoning through scaling inference computation with collaborative verification},
  author={Liang, Zhenwen and Liu, Ye and Niu, Tong and Zhang, Xiangliang and Zhou, Yingbo and Yavuz, Semih},
  journal={arXiv preprint arXiv:2410.05318},
  year={2024}
}

@article{zhang2024scaling,
  title={Scaling llm inference with optimized sample compute allocation},
  author={Zhang, Kexun and Zhou, Shang and Wang, Danqing and Wang, William Yang and Li, Lei},
  journal={arXiv preprint arXiv:2410.22480},
  year={2024}
}

@article{qu2024recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{li2022making,
  title={Making large language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{cook2024ticking,
  title={Ticking all the boxes: Generated checklists improve llm evaluation and generation},
  author={Cook, Jonathan and Rockt{\"a}schel, Tim and Foerster, Jakob and Aumiller, Dennis and Wang, Alex},
  journal={arXiv preprint arXiv:2410.03608},
  year={2024}
}

@article{ferraz2024llm,
  title={LLM self-correction with DeCRIM: Decompose, critique, and refine for enhanced following of instructions with multiple constraints},
  author={Ferraz, Thomas Palmeira and Mehta, Kartik and Lin, Yu-Hsiang and Chang, Haw-Shiuan and Oraby, Shereen and Liu, Sijia and Subramanian, Vivek and Chung, Tagyoung and Bansal, Mohit and Peng, Nanyun},
  journal={arXiv preprint arXiv:2410.06458},
  year={2024}
}

@inproceedings{goucritic,
  title={CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Duan, Nan and Chen, Weizhu and others},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}


@article{lee2025evolving,
  title={Evolving Deeper LLM Thinking},
  author={Lee, Kuang-Huei and Fischer, Ian and Wu, Yueh-Hua and Marwood, Dave and Baluja, Shumeet and Schuurmans, Dale and Chen, Xinyun},
  journal={arXiv preprint arXiv:2501.09891},
  year={2025}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{ren2023self,
  title={Self-evaluation improves selective generation in large language models},
  author={Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J and Lakshminarayanan, Balaji},
  booktitle={Proceedings on},
  pages={49--64},
  year={2023},
  organization={PMLR}
}

@misc{anthropic,
  author = {anthropic},
  title = {{Claude 3.5 Sonnet}},
  howpublished = "\url{https://www.anthropic.com/news/claude-3-5-sonnet}",
  year = {2024}, 
  note = "[Online; accessed Jun 20, 2024]"
}

@inproceedings{ren2022out,
  title={Out-of-distribution detection and selective generation for conditional language models},
  author={Ren, Jie and Luo, Jiaming and Zhao, Yao and Krishna, Kundan and Saleh, Mohammad and Lakshminarayanan, Balaji and Liu, Peter J},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{geifman2017selective,
  title={Selective classification for deep neural networks},
  author={Geifman, Yonatan and El-Yaniv, Ran},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chen2023adaptation,
  title={Adaptation with self-evaluation to improve selective prediction in llms},
  author={Chen, Jiefeng and Yoon, Jinsung and Ebrahimi, Sayna and Arik, Sercan O and Pfister, Tomas and Jha, Somesh},
  journal={arXiv preprint arXiv:2310.11689},
  year={2023}
}

@inproceedings{xin2021art,
  title={The art of abstention: Selective prediction and error regularization for natural language processing},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1040--1051},
  year={2021}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{song2024mind,
  title={Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models},
  author={Song, Yuda and Zhang, Hanlin and Eisenach, Carson and Kakade, Sham and Foster, Dean and Ghai, Udaya},
  journal={arXiv preprint arXiv:2412.02674},
  year={2024}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{balachandran2025inference,
  title={Inference-time scaling for complex tasks: Where we stand and what lies ahead},
  author={Balachandran, Vidhisha and Chen, Jingya and Chen, Lingjiao and Garg, Shivam and Joshi, Neel and Lara, Yash and Langford, John and Nushi, Besmira and Vineet, Vibhav and Wu, Yue and others},
  journal={arXiv preprint arXiv:2504.00294},
  year={2025}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@misc{aime24,
  title = {American Invitational Mathematics Examination},
  howpublished = {\url{https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions}},
  note = {AIME 2024 - 2025},
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{zhao2025sample,
  title={Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification},
  author={Zhao, Eric and Awasthi, Pranjal and Gollapudi, Sreenivas},
  journal={arXiv preprint arXiv:2502.01839},
  year={2025}
}

@misc{gemini25,
  title = {Gemini 2.5: Our most intelligent AI model},
  howpublished = {\url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/}},
}

@article{wang2025think,
  title={Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods},
  author={Wang, Junlin and Zhu, Shang and Saad-Falcon, Jon and Athiwaratkun, Ben and Wu, Qingyang and Wang, Jue and Song, Shuaiwen Leon and Zhang, Ce and Dhingra, Bhuwan and Zou, James},
  journal={arXiv preprint arXiv:2504.14047},
  year={2025}
}